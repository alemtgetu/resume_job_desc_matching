{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/detecting-document-similarity-with-doc2vec-f8289a9a7db7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"soc.religion.christian\", \"sci.space\", \"talk.politics.mideast\", \"rec.sport.baseball\"]\n",
    "cat_dict = {} # contains raw training dat organized by category\n",
    "cat_dict_test = {} # contains raw test data organized by category\n",
    "for cat in categories:\n",
    "    cat_dict[cat] = datasets.fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=[cat]).data\n",
    "    cat_dict_test[cat] = datasets.fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=[cat]).data\n",
    "\n",
    "cat_dict[\"soc.religion.christian\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_dict['soc.religion.christian'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def tokenize(text, stopwords, max_len=20):\n",
    "    return [token for token in gensim.utils.simple_preprocess(text, max_len=max_len) if token not in stopwords]\n",
    "\n",
    "cat_dict_tagged_train = {} # contains clean tagged training data organized by category\n",
    "cat_dict_test_clean = {} # contains un-tagged test dat orgainized by categroy\n",
    "\n",
    "offset = 0 # used for managing IDs of tagged documents\n",
    "for k, v in cat_dict.items():\n",
    "    cat_dict_tagged_train[k] = [gensim.models.doc2vec.TaggedDocument(tokenize(text, [], max_len=200), [i+offset]) for i, text in enumerate(v)]\n",
    "    offset += len(v)\n",
    "    \n",
    "offset = 0\n",
    "for k, v in cat_dict_test.items():\n",
    "    cat_dict_test_clean[k] = [tokenize(text, [], max_len=200) for i, text in enumerate(v)]\n",
    "    offset += len(v)\n",
    "    \n",
    "# Eventually contains final versions of the training data to actually train the model\n",
    "train_corpus = [taggeddoc for taggeddoc_list in list(cat_dict_tagged_train.values()) for taggeddoc in taggeddoc_list]\n",
    "train_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=30, min_count=2, epochs=40, window=2)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# look at https://en.wikipedia.org/wiki/Hyperparameter_optimization for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1941751   1.8838603   0.75064266  1.3050057   0.21004878 -0.588853\n",
      " -0.8507837  -0.5707667  -1.7714983   2.4439347  -1.3262669   1.2783276\n",
      " -0.7765641  -2.2415097  -1.5915937   0.11307847  1.9322265  -0.6980872\n",
      "  0.78167206 -0.5227096   0.24200228 -1.304511   -2.1284945   0.78711414\n",
      " -0.09894901 -1.5077336  -2.9868999   1.0842909   1.0127064  -1.0165315 ]\n",
      "{'soc.religion.christian': 398, 'sci.space': 394, 'talk.politics.mideast': 376, 'rec.sport.baseball': 397}\n"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "infered_vector_test = {} # contains, categor-wise, inferred doc vecs for each document in the test set\n",
    "for cat, docs in cat_dict_test_clean.items():\n",
    "    infered_vector_test[cat] = [model.infer_vector(doc) for doc in list(docs)]\n",
    "    metadata[cat] = len(infered_vector_test[cat])\n",
    "print(infered_vector_test['soc.religion.christian'][0])\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_to_csv(input, output_file, delimeter='\\t'):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=delimeter)\n",
    "        writer.writerows(input)\n",
    "        \n",
    "veclist_metadata = []\n",
    "veclist = []\n",
    "\n",
    "for cat in cat_dict.keys():\n",
    "    for tag in [cat]*metadata[cat]:\n",
    "        veclist_metadata.append([tag])\n",
    "    for vec in infered_vector_test[cat]:\n",
    "        veclist.append(list(vec))\n",
    "        \n",
    "write_to_csv(veclist, \"doc2vec_20Newsgroups_vectors.csv\")\n",
    "write_to_csv(veclist_metadata, \"doc2vec_20Newsgroups_vectors_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Call similarity_unseen_docs on a Doc2Vec model instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m cat_id:\n\u001b[1;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m id2 \u001b[39min\u001b[39;00m cat_id:\n\u001b[0;32m---> 15\u001b[0m         similarities_test[\u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m([\u001b[39mid\u001b[39m, id2]))] \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39msimilarity_unseen_docs(model, pair[\u001b[39m0\u001b[39m], pair[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m random\u001b[39m.\u001b[39msample(test_doc_pairs[\u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m([\u001b[39mid\u001b[39m,id2]))],\u001b[39mlen\u001b[39m(test_doc_pairs[\u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m([\u001b[39mid\u001b[39m,id2]))]))[:\u001b[39m500\u001b[39m]]\n",
      "Cell \u001b[0;32mIn [21], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m cat_id:\n\u001b[1;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m id2 \u001b[39min\u001b[39;00m cat_id:\n\u001b[0;32m---> 15\u001b[0m         similarities_test[\u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m([\u001b[39mid\u001b[39m, id2]))] \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39;49mdv\u001b[39m.\u001b[39;49msimilarity_unseen_docs(model, pair[\u001b[39m0\u001b[39;49m], pair[\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m random\u001b[39m.\u001b[39msample(test_doc_pairs[\u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m([\u001b[39mid\u001b[39m,id2]))],\u001b[39mlen\u001b[39m(test_doc_pairs[\u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m([\u001b[39mid\u001b[39m,id2]))]))[:\u001b[39m500\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1782\u001b[0m, in \u001b[0;36mKeyedVectors.similarity_unseen_docs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_unseen_docs\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1782\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCall similarity_unseen_docs on a Doc2Vec model instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Call similarity_unseen_docs on a Doc2Vec model instead."
     ]
    }
   ],
   "source": [
    "import random\n",
    "    \n",
    "cat_id = {id:cat for id, cat in enumerate(categories)} # Give each category a numerical id\n",
    "test_doc_pairs = {tuple(sorted([id,id2])):[] for id in cat_id for id2 in cat_id}\n",
    "for pair_id in test_doc_pairs:\n",
    "    # Create same-category doc pairs, e.g. (C3, C3)\n",
    "    if pair_id[0] == pair_id[1]:\n",
    "        test_doc_pairs[pair_id] = [(doc, cat_dict_test_clean[cat_id[pair_id[0]]][i]) for doc_index, doc in enumerate(list(cat_dict_test_clean[cat_id[pair_id[0]]])) for i in range(doc_index+1, len(list(cat_dict_test_clean[cat_id[pair_id[0]]])))]\n",
    "    #Create cross-category doc pairs, e.g. (C3, C4)\n",
    "    else:\n",
    "        test_doc_pairs[pair_id] = [(doc, doc2) for doc in list(cat_dict_test_clean[cat_id[pair_id[0]]]) for doc2 in list(cat_dict_test_clean[cat_id[pair_id[1]]])]\n",
    "similarities_test = {pair_id:[] for pair_id in test_doc_pairs}\n",
    "for id in cat_id:\n",
    "    for id2 in cat_id:\n",
    "        similarities_test[tuple(sorted([id, id2]))] = [model.dv.similarity_unseen_docs(model, pair[0], pair[1]) for pair in random.sample(test_doc_pairs[tuple(sorted([id,id2]))],len(test_doc_pairs[tuple(sorted([id,id2]))]))[:500]] # Create a similarity list of selected pairs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a12e877c04624be958b30f1ec8c927f186552dcee8ba212203c3dd40ba478394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
